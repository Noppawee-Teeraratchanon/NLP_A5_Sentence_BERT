{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bSz5jzj61nHc"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\earth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# import necessary library\n",
        "import math\n",
        "import re\n",
        "from random import *\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import wikipediaapi \n",
        "import re\n",
        "import time\n",
        "import datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# en for English\n",
        "wiki = wikipediaapi.Wikipedia('A4 (st124482@ait.asia)','en')\n",
        "# load Liverpool F.C. content\n",
        "raw_text = wiki.page('Liverpool F.C.')\n",
        "raw_text = raw_text.text\n",
        "# page.text"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Preprocessing\n",
        "\n",
        "### Tokenization and numericalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Liverpool Football Club is a professional football club based in Liverpool, England.The club competes in the Premier League, the top tier of English football.Founded in 1892, the club joined the Football League the following year and has played its home games at Anfield since its formation. Domestically, the club has won nineteen league titles, eight FA Cups, a record ten League Cups, one Football League Super Cup, one Sheriff of London Charity Shield and sixteen FA Community Shields.In international competitions, the club has won six European Cups, three UEFA Cups, four UEFA Super Cups—all English records—and one FIFA Club World Cup.The club established itself as a major force in domestic and European football in the 1970s and 1980s, when Bill Shankly, Bob Paisley, Joe Fagan and Kenny Dalglish, led the club to a combined 11 League titles and four European Cups.Liverpool won two further European Cups in 2005 and 2019 under the management of Rafael Benítez and Jürgen Klopp, respectively; the latter led Liverpool to a 19th league title in 2020, the club's first during the Premier League era.Liverpool is one of the most valuable and widely supported clubs in the world.The club has long-standing rivalries with Manchester United and Everton.Under management by Shankly, in 1964 the team changed from red shirts and white shorts to an all-red home strip which has been used ever since.The club's anthem is \"You'll Never Walk Alone\".The club's supporters have been involved in two major tragedies.The Heysel Stadium disaster, where escaping fans were pressed against a collapsing wall at the 1985 European Cup Final in Brussels, resulted in 39 deaths.Most of these were Italians and Juventus fans.Liverpool were given a six-year ban from European competition, and all other English clubs received a five-year ban.The Hillsborough disaster in 1989, where 97 Liverpool supporters died in a crush against perimeter fencing, led to the elimination of fenced standing terraces in favour of all-seater stadiums in the top two tiers of English football.Prolonged campaigning for justice saw further coroners inquests, commissions and independent panels that ultimately exonerated the fans.History Liverpool F. C.was founded following a dispute between the Everton committee and John Houlding, club president and owner of the land at Anfield.After eight years at the stadium, Everton relocated to Goodison Park in 1892 and Houlding founded Liverpool F. C.to play at Anfield.Originally named \"Everton F. C.and Athletic Grounds Ltd\" (Everton Athletic for short), the club became Liverpool F. C.in March 1892 and gained official recognition three months later, after The Football Association refused to recognise the club as Everton. Liverpool played their first match on 1 September 1892, a pre-season friendly match against Rotherham Town, which they won 7–1.The team Liverpool fielded against Rotherham was composed entirely of Scottish players—the players who came from Scotland to play in England in those days were known as the Scotch Professors.Manager John McKenna had recruited the players after a scouting trip to Scotland—so they became known as the \"team of Macs\".The team won the Lancashire League in its debut season and joined the Football League Second Division at the start of the 1893–94 season.After the club was promoted to the First Division in 1896, Tom Watson was appointed manager.He led Liverpool to its first league title in 1901, before winning it again in 1906. Liverpool reached its first FA Cup Final in 1914, losing 1–0 to Burnley.It won consecutive League championships in 1922 and 1923, but did not win another trophy until the 1946–47 season, when the club won the First Division for a fifth time under the control of ex-West Ham United centre half George Kay.Liverpool suffered its second Cup Final defeat in 1950, playing against Arsenal.The club was relegated to the Second Division in the 1953–54 season.Soon after Liverpool lost 2–1 to non-league Worcester City in the 1958–59 FA Cup, Bill Shankly was appointed manager.Upon his arrival he released 24 players and converted a boot storage room at Anfield into a room where the coaches could discuss strategy; here, Shankly and other \"Boot Room\" members Joe Fagan, Reuben Bennett, and Bob Paisley began reshaping the team.The club was promoted back into the First Division in 1962 and won it in 1964, for the first time in 17 years.In 1965, the club won its first FA Cup.In 1966, the club won the First Division but lost to Borussia Dortmund in the European Cup Winners' Cup final.Liverpool won both the League and the UEFA Cup during the 1972–73 season, and the FA Cup again a year later.Shankly retired soon afterwards and was replaced by his assistant, Bob Paisley.In 1976, Paisley's second season as manager, the club won another League and UEFA Cup double.The following season, the club retained the League title and won the European Cup for the first time, but it lost in the 1977 FA Cup Final.Liverpool retained the European Cup in 1978 and regained the First Division title in 1979.During Paisley's nine seasons as manager Liverpool won 20 trophies, including three European Cups, a UEFA Cup, six League titles and three consecutive League Cups; the only domestic trophy he did not win was the FA Cup.Paisley retired in 1983 and was replaced by his assistant, Joe Fagan.Liverpool won the League, League Cup and European Cup in Fagan's first season, becoming the first English side to win three trophies in a season.Liverpool reached the European Cup final again in 1985, against Juventus at the Heysel Stadium.Before kick-off, Liverpool fans breached a fence that separated the two groups of supporters and charged the Juventus fans.The resulting weight of people caused a retaining wall to collapse, killing 39 fans, mostly Italians.The incident became known as the Heysel Stadium disaster.The match was played in spite of protests by both managers, and Liverpool lost 1–0 to Juventus.As a result of the tragedy, English clubs were banned from participating in European competition for five years; Liverpool received a ten-year ban, which was later reduced to six years.Fourteen Liverpool fans received convictions for involuntary manslaughter. Fagan had announced his retirement just before the disaster and Kenny Dalglish was appointed as player-manager.During his tenure, the club won another three league titles and two FA Cups, including a League and Cup \"Double\" in the 1985–86 season.Liverpool's success was overshadowed by the Hillsborough disaster: in an FA Cup semi-final against Nottingham Forest on 15 April 1989, hundreds of Liverpool fans were crushed against perimeter fencing.Ninety-four fans died that day; the 95th victim died in hospital from his injuries four days later, the 96th died nearly four years later, without regaining consciousness, and the 97th, Andrew Devine, died of injuries sustained in the disaster in 2021.After the Hillsborough disaster there was a government review of stadium safety.The resulting Taylor Report paved the way for legislation that required top-division teams to have all-seater stadiums.The report ruled that the main reason for the disaster was overcrowding due to a failure of police control.Liverpool was involved in the closest finish to a league season during the 1988–89 season.Liverpool finished equal with Arsenal on both points and goal difference, but lost the title on total goals scored when Arsenal scored the final goal in the last minute of the season. Dalglish cited the Hillsborough disaster and its repercussions as the reason for his resignation in 1991; he was replaced by former player Graeme Souness.Under his leadership Liverpool won the 1992 FA Cup Final, but their league performances slumped, with two consecutive sixth-place finishes, eventually resulting in his dismissal in January 1994.Souness was replaced by Roy Evans, and Liverpool went on to win the 1995 Football League Cup Final.While they made some title challenges under Evans, third-place finishes in 1996 and 1998 were the best they could manage, and so Gérard Houllier was appointed co-manager in the 1998–99 season, and became the sole manager in November 1998 after Evans resigned.In 2001, Houllier's second full season in charge, Liverpool won a \"treble\": the FA Cup, League Cup and UEFA Cup.Houllier underwent major heart surgery during the 2001–02 season and Liverpool finished second in the League, behind Arsenal.They won a further League Cup in 2003, but failed to mount a title challenge in the two seasons that followed.Houllier was replaced by Rafael Benítez at the end of the 2003–04 season.Despite finishing fifth in Benítez's first season, Liverpool won the 2004–05 UEFA Champions League, beating A. C.Milan 3–2 in a penalty shootout after the match ended with a score of 3–3.The following season, Liverpool finished third in the Premier League and won the 2006 FA Cup Final, beating West Ham United in a penalty shootout after the match finished 3–3.American businessmen George Gillett and Tom Hicks became the owners of the club during the 2006–07 season, in a deal which valued the club and its outstanding debts at £218. 9 million.The club reached the 2007 UEFA Champions League Final against Milan, as it had in 2005, but lost 2–1.During the 2008–09 season Liverpool achieved 86 points, its then-highest Premier League points total, prior to the record-breaking 2018–19 season, and finished as runners up to Manchester United. In the 2009–10 season, Liverpool finished seventh in the Premier League and failed to qualify for the Champions League.Benítez subsequently left by mutual consent and was replaced by Fulham manager Roy Hodgson.At the start of the 2010–11 season Liverpool was on the verge of bankruptcy and the club's creditors asked the High Court to allow the sale of the club, overruling the wishes of Hicks and Gillett.John W.Henry, owner of the Boston Red Sox and of Fenway Sports Group, bid successfully for the club and took ownership in October 2010.Poor results during the start of that season led to Hodgson leaving the club by mutual consent and former player and manager Kenny Dalglish taking over.In the 2011–12 season, Liverpool secured a record 8th League Cup success and reached the FA Cup final, but finished in eighth position, the worst league finish in 18 years; this led to the sacking of Dalglish.He was replaced by Brendan Rodgers, whose Liverpool team in the 2013–14 season mounted an unexpected title charge to finish second behind champions Manchester City and subsequently return to the Champions League, scoring 101 goals in the process, the most since the 106 scored in the 1895–96 season.Following a disappointing 2014–15 season, where Liverpool finished sixth in the league, and a poor start to the following campaign, Rodgers was sacked in October 2015. Rodgers was replaced by Jürgen Klopp.Liverpool reached the finals of the Football League Cup and UEFA Europa League in Klopp's first season, finishing as runner-up in both competitions.The club finished second in the 2018–19 season with 97 points (surpassing the 86 points gained during the 2008–09 season), losing only one game: a points record for a non-title winning side.Klopp took Liverpool to successive Champions League finals in 2018 and 2019, with the club defeating Tottenham Hotspur 2–0 to win the 2019 UEFA Champions League Final.Liverpool beat Flamengo of Brazil in the final 1–0 to win the FIFA Club World Cup for the first time.Liverpool then went on to win the 2019–20 Premier League, winning their first top-flight league title in thirty years.The club set multiple records in the season, including winning the league with seven games remaining making it the earliest any team has ever won the title, amassing a club record 99 points, and achieving a joint-record 32 wins in a top-flight season.In January 2024, Klopp announced that he would leave the club at the end of the season.Colours and badge For much of Liverpool's history, its home colours have been all red.When the club was founded in 1892, blue and white quartered shirts were used until the club adopted the city's colour of red in 1896.The city's symbol of the liver bird was adopted as the club's badge (or crest, as it is sometimes known) in 1901, although it was not incorporated into the kit until 1955.Liverpool continued to wear red shirts and white shorts until 1964 when manager Bill Shankly decided to change to an all-red strip.Liverpool played in all red for the first time against Anderlecht, as Ian St John recalled in his autobiography: He [Shankly] thought the colour scheme would carry psychological impact – red for danger, red for power.He came into the dressing room one day and threw a pair of red shorts to Ronnie Yeats.\"Get into those shorts and let's see how you look\", he said.\"Christ, Ronnie, you look awesome, terrifying.You look 7 ft tall. \" \"Why not go the whole hog, boss?\" I suggested.\"Why not wear red socks? Let's go out all in red. \" Shankly approved and an iconic kit was born. The Liverpool away strip has more often than not been all yellow or white shirts and black shorts, but there have been several exceptions.An all grey kit was introduced in 1987, which was used until the 1991–92 centenary season when it was replaced by a combination of green shirts and white shorts.After various colour combinations in the 1990s, including gold and navy, bright yellow, black and grey, and ecru, the club alternated between yellow and white away kits until the 2008–09 season, when it re-introduced the grey kit.A third kit is designed for European away matches, though it is also worn in domestic away matches on occasions when the current away kit clashes with a team's home kit.Between 2012 and 2015, the kits were designed by Warrior Sports, who became the club's kit providers at the start of the 2012–13 season.In February 2015, Warrior's parent company New Balance announced it would be entering the global football market, with teams sponsored by Warrior now being outfitted by New Balance.The only other branded shirts worn by the club were made by Umbro until 1985, when they were replaced by Adidas, who produced the kits until 1996 when Reebok took over.They produced the kits for 10 years before Adidas made the kits from 2006 to 2012.Nike became the club's official kit supplier at the start of the 2020–21 season.Liverpool was the first English professional club to have a sponsor's logo on its shirts, after agreeing a deal with Hitachi in 1979.However for the first few years of the deal, broadcasting rules meant that sponsors logos could not be shown on shirts for televised matches. Since then the club has been sponsored by Crown Paints, Candy, Carlsberg and Standard Chartered.The contract with Carlsberg, which was signed in 1992, was the longest-lasting agreement in English top-flight football.The association with Carlsberg ended at the start of the 2010–11 season, when Standard Chartered Bank became the club's sponsor. The Liverpool badge is based on the city's liver bird symbol, which in the past had been placed inside a shield.In 1977, a red liver bird standing on a football (blazoned as \"Statant upon a football a Liver Bird wings elevated and addorsed holding in the beak a piece of seaweed gules\") was granted as a heraldic badge by the College of Arms to the English Football League intended for use by Liverpool.However, Liverpool never made use of this badge.In 1992, to commemorate the centennial of the club, a new badge was commissioned, including a representation of the Shankly Gates.The next year twin flames were added at either side, symbolic of the Hillsborough memorial outside Anfield, where an eternal flame burns in memory of those who died in the Hillsborough disaster.In 2012, Warrior Sports' first Liverpool kit removed the shield and gates, returning the badge to what had adorned Liverpool shirts in the 1970s; the flames were moved to the back collar of the shirt, surrounding the number 96 for the number who died at Hillsborough.Sponsorship Stadium Anfield was built in 1884 on land adjacent to Stanley Park.Situated 2 miles (3 km) from Liverpool city centre, it was originally used by Everton before the club moved to Goodison Park after a dispute over rent with Anfield owner John Houlding.Left with an empty ground, Houlding founded Liverpool in 1892 and the club has played at Anfield ever since.The capacity of the stadium at the time was 20,000, although only 100 spectators attended Liverpool's first match at Anfield. The Kop was built in 1906 due to the high turnout for matches and was called the Oakfield Road Embankment initially.Its first game was on 1 September 1906 when the home side beat Stoke City 1–0.In 1906 the banked stand at one end of the ground was formally renamed the Spion Kop after a hill in KwaZulu-Natal.The hill was the site of the Battle of Spion Kop in the Second Boer War, where over 300 men of the Lancashire Regiment died, many of them from Liverpool.At its peak, the stand could hold 28,000 spectators and was one of the largest single-tier stands in the world.Many stadiums in England had stands named after Spion Kop, but Anfield's was the largest of them at the time; it could hold more supporters than some entire football grounds. Anfield could accommodate more than 60,000 supporters at its peak and had a capacity of 55,000 until the 1990s, when, following recommendations from the Taylor Report, all clubs in the Premier League were obliged to convert to all-seater stadiums in time for the 1993–94 season, reducing its capacity to 45,276.The findings of the report precipitated the redevelopment of the Kemlyn Road Stand, which was rebuilt in 1992, coinciding with the centenary of the club, and was known as the Centenary Stand until 2017 when it was renamed the Kenny Dalglish Stand.An extra tier was added to the Anfield Road end in 1998, which further increased the capacity of the ground but gave rise to problems when it was opened.A series of support poles and stanchions were inserted to give extra stability to the top tier of the stand after movement of the tier was reported at the start of the 1999–2000 season. Because of restrictions on expanding the capacity at Anfield, Liverpool announced plans to move to the proposed Stanley Park Stadium in May 2002.Planning permission was granted in July 2004, and in September 2006, Liverpool City Council agreed to grant Liverpool a 999-year lease on the proposed site.Following the takeover of the club by George Gillett and Tom Hicks in February 2007, the proposed stadium was redesigned.The new design was approved by the Council in November 2007.The stadium was scheduled to open in August 2011 and would hold 60,000 spectators, with HKS, Inc.contracted to build the stadium.Construction was halted in August 2008, as Gillett and Hicks had difficulty in financing the £300 million needed for the development.In October 2012, BBC Sport reported that Fenway Sports Group, the new owners of Liverpool FC, had decided to redevelop their current home at Anfield stadium, rather than building a new stadium in Stanley Park.As part of the redevelopment the capacity of Anfield was to increase from 45,276 to approximately 60,000 and would cost approximately £150m.When construction was completed on the new Main stand the capacity of Anfield was increased to 54,074.This £100 million expansion added a third tier to the stand.This was all part of a £260 million project to improve the Anfield area.Jürgen Klopp the manager at the time described the stand as \"impressive. \"In June 2021, it was reported that Liverpool Council had given planning permission for the club to renovate and expand the Anfield Road stand, boosting the capacity by around 7,000 and taking the overall capacity at Anfield to 61,000.The expansion, which is estimated to cost £60m, was described as \"a huge milestone\" by managing director Andy Hughes, and would also see rail seating being trialled in the Kop for the 2021–22 Premier League season.Support Liverpool is one of the best supported clubs in the world.The club states that its worldwide fan base includes 300 officially recognised Supporters Clubs in 100 different countries.Notable groups include Spirit of Shankly.The club takes advantage of this support through its worldwide summer tours, which has included playing in front of 101,000 in Michigan, U. S. , and 95,000 in Melbourne, Australia.Liverpool fans often refer to themselves as Kopites, a reference to the fans who once stood, and now sit, on the Kop at Anfield.In 2008 a group of fans decided to form a splinter club, A. F. C.Liverpool, to play matches for fans who had been priced out of watching Premier League football. The song \"You'll Never Walk Alone\", originally from the Rodgers and Hammerstein musical Carousel and later recorded by Liverpool musicians Gerry and the Pacemakers, is the club's anthem and has been sung by the Anfield crowd since the early 1960s.It has since gained popularity among fans of other clubs around the world.The song's title adorns the top of the Shankly Gates, which were unveiled on 2 August 1982 in memory of former manager Bill Shankly.The \"You'll Never Walk Alone\" portion of the Shankly Gates is also reproduced on the club's badge.The club's supporters have been involved in two stadium disasters.The first was the 1985 Heysel Stadium disaster, in which 39 people, mostly Italians and Juventus supporters, were killed.They were confined to a corner by Liverpool fans who had charged in their direction; the weight of the cornered fans caused a wall to collapse.UEFA laid the blame for the incident solely on the Liverpool supporters, and banned all English clubs from European competition for five years.Liverpool was banned for an additional year, preventing it from participating in the 1990–91 European Cup, even though it won the League in 1990.Twenty-seven fans were arrested on suspicion of manslaughter and were extradited to Belgium in 1987 to face trial.In 1989, after a five-month trial in Belgium, 14 Liverpool fans were given three-year sentences for involuntary manslaughter; half of the terms were suspended. The second disaster took place during an FA Cup semi-final between Liverpool and Nottingham Forest at Hillsborough Stadium, Sheffield, on 15 April 1989.Ninety-seven Liverpool fans died as a consequence of overcrowding at the Leppings Lane end, in what became known as the Hillsborough disaster.In the following days, The Sun's coverage of the event spread falsehoods, particularly an article entitled \"The Truth\" that  claimed that Liverpool fans had robbed the dead and had urinated on and attacked the police.Subsequent investigations proved the allegations false, leading to a boycott of the newspaper by Liverpool fans across the city and elsewhere; many still refuse to buy The Sun 30 years later.Many support organisations were set up in the wake of the disaster, such as the Hillsborough Justice Campaign, which represents bereaved families, survivors and supporters in their efforts to secure justice.Rivalries Liverpool's longest-established rivalry is with fellow Liverpool team Everton, against whom they contest the Merseyside derby.The rivalry stems from Liverpool's formation and the dispute with Everton officials and the then owners of Anfield.The Merseyside derby is one of the few local derbies which do not enforce fan segregation, and hence has been known as the \"friendly derby\".Since the mid-1980s, the rivalry has intensified both on and off the field and, since the inception of the Premier League in 1992, the Merseyside derby has had more players sent off than any other Premier League game.It has been referred to as \"the most ill-disciplined and explosive fixture in the Premier League\".In terms of support within the city, the number of Liverpool fans outweighs Everton supporters by a ratio of 2:1. Liverpool's rivalry with Manchester United stems from the cities' competition in the Industrial Revolution of the 19th century.Connected by the world's first inter-city railway, by road Liverpool and Manchester are separated by approximately 30 miles (48 km) along the East Lancs Road.Ranked the two biggest clubs in England by France Football magazine, Liverpool and Manchester United are the most successful English teams in both domestic and international competitions, and both clubs have a global fanbase.Viewed as one of the biggest rivalries in world football, it is considered the most famous fixture in English football.The two clubs alternated as champions between 1964 and 1967, and Manchester United became the first English team to win the European Cup in 1968, followed by Liverpool's four European Cup victories.Despite the 39 league titles and nine European Cups between them the two rivals have rarely been successful at the same time – Liverpool's run of titles in the 1970s and 1980s coincided with Manchester United's 26-year title drought, and United's success in the Premier League-era likewise coincided with Liverpool's 30-year title drought, and the two clubs have finished first and second in the league only five times.Such is the rivalry between the clubs they rarely do transfer business with each other.The last player to be transferred between the two clubs was Phil Chisnall, who moved to Liverpool from Manchester United in 1964.Ownership and finances As the owner of Anfield and founder of Liverpool, John Houlding was the club's first chairman, a position he held from its founding in 1892 until 1904.John McKenna took over as chairman after Houlding's departure.McKenna subsequently became President of the Football League.The chairmanship changed hands many times before John Smith, whose father was a shareholder of the club, took up the role in 1973.He oversaw the most successful period in Liverpool's history before stepping down in 1990.His successor was Noel White who became chairman in 1990.In August 1991 David Moores, whose family had owned the club for more than 50 years, became chairman.His uncle John Moores was also a shareholder at Liverpool and was chairman of Everton from 1961 to 1973.Moores owned 51 percent of the club, and in 2004 expressed his willingness to consider a bid for his shares in Liverpool. Moores eventually sold the club to American businessmen George Gillett and Tom Hicks on 6 February 2007.The deal valued the club and its outstanding debts at £218. 9 million.The pair paid £5,000 per share, or £174. 1m for the total shareholding and £44. 8m to cover the club's debts.Disagreements between Gillett and Hicks, and the fans' lack of support for them, resulted in the pair looking to sell the club.Martin Broughton was appointed chairman of the club on 16 April 2010 to oversee its sale.In May 2010, accounts were released showing the holding company of the club to be £350m in debt (due to leveraged takeover) with losses of £55m, causing auditor KPMG to qualify its audit opinion.The group's creditors, including the Royal Bank of Scotland, took Gillett and Hicks to court to force them to allow the board to proceed with the sale of the club, the major asset of the holding company.A High Court judge, Mr Justice Floyd, ruled in favour of the creditors and paved the way for the sale of the club to Fenway Sports Group (formerly New England Sports Ventures), although Gillett and Hicks still had the option to appeal.Liverpool was sold to Fenway Sports Group on 15 October 2010 for £300m. Liverpool has been described as a global brand; a 2010 report valued the club's trademarks and associated intellectual property at £141m, an increase of £5m on the previous year.Liverpool was given a brand rating of AA (Very Strong).In April 2010 business magazine Forbes ranked Liverpool as the sixth most valuable football team in the world, behind Manchester United, Real Madrid, Arsenal, Barcelona and Bayern Munich; they valued the club at $822m (£532m), excluding debt.Accountants Deloitte ranked Liverpool eighth in the Deloitte Football Money League, which ranks the world's football clubs in terms of revenue.Liverpool's income in the 2009–10 season was €225. 3m.According to a 2018 report by Deloitte, the club had an annual revenue of €424. 2 million for the previous year, and Forbes valued the club at $1. 944 billion.In 2018, annual revenue increased to €513. 7 million, and Forbes valued the club at $2. 183 billion.In 2019 revenue increased to €604 million (£533 million) according to Deloitte, with the club breaching the half a billion pounds mark. In April 2020, the owners of the club came under fire from fans and the media for deciding to furlough all non-playing staff during the COVID-19 pandemic.In response to this, the club made a U-turn on the decision and apologised for their initial decision.In April 2021 Forbes valued the club at $4. 1 billion, a two-year increase of 88%, making it the world's fifth-most-valuable football club.Based on the latest rankings by Forbes, as of May 2023, Liverpool is ranked as the fourth most valuable football club in the world, behind Real Madrid, Manchester United and Barcelona; they valued the club at $5. 29 billion, an increase of 19% from 2022.Liverpool in the media Liverpool featured in the first edition of BBC's Match of the Day, which screened highlights of their match against Arsenal at Anfield on 22 August 1964.The first football match to be televised in colour was between Liverpool and West Ham United, broadcast live in March 1967.Liverpool fans featured in the Pink Floyd song \"Fearless\", in which they sang excerpts from \"You'll Never Walk Alone\".To mark the club's appearance in the 1988 FA Cup Final, Liverpool released the \"Anfield Rap\", a song featuring John Barnes and other members of the squad. A docudrama on the Hillsborough disaster, written by Jimmy McGovern, was screened in 1996.It featured Christopher Eccleston as Trevor Hicks, who lost two teenage daughters in the disaster, went on to campaign for safer stadiums and helped to form the Hillsborough Families Support Group.Liverpool featured in the 2001 film The 51st State, in which ex-hitman Felix DeSouza (Robert Carlyle) is a keen supporter of the team and the last scene takes place at a match between Liverpool and Manchester United.The club also featured in the 1984 children's television show Scully, about a young boy who tries to gain a trial with Liverpool.In the Doctor Who episode \"The Halloween Apocalypse\", aired in October 2021, features The Doctor (played by Jodie Whittaker) exiting the TARDIS outside Anfield as she exclaims: \"Liverpool? Anfield! Klopp era, classic!\".Players First-team squad As of 31 January 2024Note: Flags indicate national team as defined under FIFA eligibility rules.Players may hold more than one non-FIFA nationality.Out on loan Note: Flags indicate national team as defined under FIFA eligibility rules.Players may hold more than one non-FIFA nationality.Reserves and Academy Former players Player records Club captains Since the establishment of the club in 1892, 46 players have been club captain of Liverpool F. C.Andrew Hannah became the first captain of the club after Liverpool separated from Everton and formed its own club.Alex Raisbeck, who was club captain from 1899 to 1909, was the longest serving captain before being overtaken by Steven Gerrard who served 12 seasons as Liverpool captain starting from the 2003–04 season.The present captain is Virgil van Dijk, who in the 2023–24 season replaced Jordan Henderson who moved to Al-Ettifaq.Player of the season Club officials Honours Liverpool's first trophy was the Lancashire League, which it won in the club's first season.In 1901, the club won its first League title, while the nineteenth and most recent was in 2020.Its first success in the FA Cup was in 1965.In terms of the number of trophies won, Liverpool's most successful decade was the 1980s, when the club won six League titles, two FA Cups, four League Cups, one Football League Super Cup, five Charity Shields (one shared) and two European Cups.In 2020, Liverpool became the first English club to have won a League title in eight different decades. The club has accumulated more top-flight wins and points than any other English team.Liverpool also has the highest average league finishing position (3. 3) for the 50-year period to 2015 and second-highest average league finishing position for the period 1900–1999 after Arsenal, with an average league placing of 8. 7. Liverpool is the most successful British club in international football with fourteen trophies, having won the European Cup/UEFA Champions League, UEFA's premier club competition, six times, an English record and only surpassed by Real Madrid and A. C.Milan.Liverpool's fifth European Cup win, in 2005, meant that the club was awarded the trophy permanently and was also awarded a multiple-winner badge.Liverpool also hold the English record of three wins in the UEFA Cup, UEFA's secondary club competition.Liverpool also hold the English record of four wins in the UEFA Super Cup.In 2019, the club won the FIFA Club World Cup for the first time, and also became the first English club to win the international treble of Club World Cup, Champions League and UEFA Super Cup.\n"
          ]
        }
      ],
      "source": [
        "# clean data\n",
        "raw_text = raw_text.replace('\\n\\n', ' ')\n",
        "raw_text = raw_text.replace('\\n', ' ')\n",
        "raw_text = raw_text.replace('.', '. ')\n",
        "raw_text = raw_text.replace('.  ', '.')\n",
        "\n",
        "# delete some text that it not be a sentence\n",
        "# find the index of \"Minor titles\"\n",
        "start_index = raw_text.find(\"Minor titles\")\n",
        "\n",
        "# If \"Minor titles\" is found, slice the string to remove the content from that point onward\n",
        "if start_index != -1:\n",
        "    cleaned_text = raw_text[:start_index]\n",
        "\n",
        "    # Print the cleaned text\n",
        "    print(cleaned_text)\n",
        "else:\n",
        "    print(\"String 'Minor titles' not found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# split sentence from raw text\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(cleaned_text)\n",
        "sentences = list(doc.sents)\n",
        "\n",
        "# lower case, and clean all the symbols\n",
        "text = [x.text.lower() for x in sentences]\n",
        "# clean the data\n",
        "text = [re.sub(\"f. c.\", 'fc ', x) for x in text]\n",
        "text = [re.sub(\"[,!.?\\\\-;]\", '', x) for x in text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# making vocab list\n",
        "word_list = list(set(\" \".join(text).split()))\n",
        "word2id   = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, w in enumerate(word_list):\n",
        "    word2id[w] = i + 4 #reserve the first 0-3 for CLS, PAD\n",
        "    id2word    = {i:w for i, w  in enumerate(word2id)}\n",
        "    vocab_size = len(word2id)\n",
        "\n",
        "# convert the word to numeric of each sentence\n",
        "token_list = list()\n",
        "for sentence in text:\n",
        "    arr = [word2id[word] for word in sentence.split()]\n",
        "    token_list.append(arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'£218'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "id2word[1270]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "213"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(sentences)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# assign some hyperparameter\n",
        "batch_size = 6\n",
        "max_mask   = 5 \n",
        "max_len    = 200 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create batch that has half for positive and another half for negative\n",
        "def make_batch():\n",
        "    batch = []\n",
        "    positive = negative = 0\n",
        "    while positive != batch_size / 2 or negative != batch_size / 2: # if positive is half, negative is also half\n",
        "        \n",
        "        #randomly choose two sentence\n",
        "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences))\n",
        "        tokens_a, tokens_b            = token_list[tokens_a_index], token_list[tokens_b_index]\n",
        "        \n",
        "        #1. token embedding - add CLS and SEP on starting and ending of sentence respectively\n",
        "        input_ids = [word2id['[CLS]']] + tokens_a + [word2id['[SEP]']] + tokens_b + [word2id['[SEP]']]\n",
        "        \n",
        "        #2. segment embedding - which sentence is 0 (first sentence) and 1 (second sentence)\n",
        "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
        "        \n",
        "        #3 masking\n",
        "        n_pred = min(max_mask, max(1, int(round(len(input_ids) * 0.15))))\n",
        "        #get all the pos excluding CLS and SEP\n",
        "        candidates_masked_pos = [i for i, token in enumerate(input_ids) if token != word2id['[CLS]'] \n",
        "                                 and token != word2id['[SEP]']]\n",
        "        shuffle(candidates_masked_pos)\n",
        "        masked_tokens, masked_pos = [], [] #compare the output with masked_tokens\n",
        "        #simply loop and mask accordingly\n",
        "        for pos in candidates_masked_pos[:n_pred]:\n",
        "            masked_pos.append(pos)\n",
        "            masked_tokens.append(input_ids[pos])\n",
        "            if random() < 0.1:  #10% replace with random token\n",
        "                index = randint(0, vocab_size - 1)\n",
        "                input_ids[pos] = word2id[id2word[index]]\n",
        "            elif random() < 0.8:  #80 replace with [MASK]\n",
        "                input_ids[pos] = word2id['[MASK]']\n",
        "            else: \n",
        "                pass\n",
        "            \n",
        "        #4. pad the sentence to the max length\n",
        "        n_pad = max_len - len(input_ids)\n",
        "        input_ids.extend([0] * n_pad)\n",
        "        segment_ids.extend([0] * n_pad)\n",
        "        \n",
        "        #5. pad the mask tokens to the max length\n",
        "        if max_mask > n_pred:\n",
        "            n_pad = max_mask - n_pred\n",
        "            masked_tokens.extend([0] * n_pad)\n",
        "            masked_pos.extend([0] * n_pad)\n",
        "        \n",
        "        #6. check whether is positive or negative\n",
        "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size / 2:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # True = it is the next sentence\n",
        "            positive += 1\n",
        "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size / 2:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False])\n",
        "            negative += 1\n",
        "        \n",
        "    return batch\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check the make_batch to ensure that it work correctly\n",
        "batch = make_batch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([6, 200]),\n",
              " torch.Size([6, 200]),\n",
              " torch.Size([6, 5]),\n",
              " torch.Size([6, 5]),\n",
              " tensor([0, 0, 0, 1, 1, 1]))"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_ids.shape, segment_ids.shape, masked_tokens.shape, masked_pos.shape, isNext"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.1 Embedding\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
        "        self.pos_embed = nn.Embedding(max_len, d_model)      # position embedding\n",
        "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, seg):\n",
        "        #x, seg: (bs, len)\n",
        "        seq_len = x.size(1)\n",
        "        pos = torch.arange(seq_len, dtype=torch.long)\n",
        "        pos = pos.unsqueeze(0).expand_as(x)  # (len,) -> (bs, len)\n",
        "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
        "        return self.norm(embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2 Attention mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "s1PGksqBNuZM"
      },
      "outputs": [],
      "source": [
        "def get_attn_pad_mask(seq_q, seq_k):\n",
        "    batch_size, len_q = seq_q.size()\n",
        "    batch_size, len_k = seq_k.size()\n",
        "    # eq(zero) is PAD token\n",
        "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
        "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing the attention mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([6, 200, 200])\n"
          ]
        }
      ],
      "source": [
        "print(get_attn_pad_mask(input_ids, input_ids).shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.3 Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.enc_self_attn = MultiHeadAttention()\n",
        "        self.pos_ffn       = PoswiseFeedForwardNet()\n",
        "\n",
        "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
        "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
        "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
        "        return enc_outputs, attn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define the scaled dot attention, to be used inside the multihead attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
        "        attn = nn.Softmax(dim=-1)(scores)\n",
        "        context = torch.matmul(attn, V)\n",
        "        return context, attn "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define the parameters first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_layers = 6    # number of Encoder of Encoder Layer\n",
        "n_heads  = 8    # number of heads in Multi-Head Attention\n",
        "d_model  = 768  # Embedding Size\n",
        "d_ff = 768 * 4  # 4*d_model, FeedForward dimension\n",
        "d_k = d_v = 64  # dimension of K(=Q), V\n",
        "n_segments = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is the Multiheadattention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
        "        residual, batch_size = Q, Q.size(0)\n",
        "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
        "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
        "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
        "\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
        "\n",
        "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
        "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
        "        return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is the PoswiseFeedForwardNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n",
        "        return self.fc2(F.gelu(self.fc1(x)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.4 Putting them together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "OZ0TJ84W4SZw"
      },
      "outputs": [],
      "source": [
        "class BERT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERT, self).__init__()\n",
        "        self.embedding = Embedding()\n",
        "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "        self.activ = nn.Tanh()\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.classifier = nn.Linear(d_model, 2)\n",
        "        # decoder is shared with embedding layer\n",
        "        embed_weight = self.embedding.tok_embed.weight\n",
        "        n_vocab, n_dim = embed_weight.size()\n",
        "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
        "        self.decoder.weight = embed_weight\n",
        "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
        "\n",
        "    def forward(self, input_ids, segment_ids, masked_pos):\n",
        "        output = self.embedding(input_ids, segment_ids)\n",
        "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
        "        for layer in self.layers:\n",
        "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
        "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
        "        \n",
        "        # 1. predict next sentence\n",
        "        # it will be decided by first token(CLS)\n",
        "        h_pooled   = self.activ(self.fc(output[:, 0])) # [batch_size, d_model]\n",
        "        logits_nsp = self.classifier(h_pooled) # [batch_size, 2]\n",
        "\n",
        "        # 2. predict the masked token\n",
        "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
        "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
        "        h_masked  = self.norm(F.gelu(self.linear(h_masked)))\n",
        "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n",
        "\n",
        "        return logits_lm, logits_nsp"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UAG3SEP4UbU",
        "outputId": "bc6f202f-df37-4fac-843c-fb86bdb777b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 00 loss = 99.312897\n",
            "Epoch: 100 loss = 5.052397\n",
            "Epoch: 200 loss = 4.202962\n",
            "Epoch: 300 loss = 4.552831\n",
            "Epoch: 400 loss = 3.918600\n",
            "Time: 12m 21s\n"
          ]
        }
      ],
      "source": [
        "num_epoch = 500\n",
        "model = BERT()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "batch = make_batch()\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
        "\n",
        "best_loss = float('inf')\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epoch):\n",
        "    optimizer.zero_grad()\n",
        "    logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)    \n",
        "\n",
        "    #1. mlm loss\n",
        "    #logits_lm.transpose: (bs, vocab_size, max_mask) vs. masked_tokens: (bs, max_mask)\n",
        "    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
        "    loss_lm = (loss_lm.float()).mean()\n",
        "    #2. nsp loss\n",
        "    #logits_nsp: (bs, 2) vs. isNext: (bs, )\n",
        "    loss_nsp = criterion(logits_nsp, isNext) # for sentence classification\n",
        "    \n",
        "    #3. combine loss\n",
        "    loss = loss_lm + loss_nsp\n",
        "    if loss < best_loss:\n",
        "        best_loss = loss\n",
        "        torch.save(model.state_dict(), 'models/best-bert-model.pt')\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch:', '%02d' % (epoch), 'loss =', '{:.6f}'.format(loss))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "end_time = time.time()\n",
        "epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "print(f'Time: {epoch_mins}m {epoch_secs}s')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Inference\n",
        "\n",
        "Since our dataset is very small, it won't work very well, but just for the sake of demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[CLS]', 'liverpool', 'featured', 'in', 'the', '2001', 'film', 'the', '51st', 'state', '[MASK]', 'which', 'exhitman', 'felix', 'desouza', '(robert', 'carlyle)', 'is', 'a', 'keen', 'supporter', 'of', 'the', 'team', '[MASK]', 'the', 'last', 'scene', 'takes', 'place', 'at', '[MASK]', 'match', 'between', 'liverpool', 'and', 'manchester', 'united', '[SEP]', 'liverpool', 'suffered', 'its', 'second', 'cup', 'final', 'defeat', 'finances', '1950', 'playing', 'against', '[MASK]', '[SEP]']\n",
            "masked tokens (words) :  ['and', 'in', 'in', 'a', 'arsenal']\n",
            "masked tokens list :  [669, 775, 775, 908, 1145]\n",
            "predict masked tokens (words) :  ['[PAD]', '[PAD]', 'of', '[PAD]', 'of']\n",
            "predict masked tokens list :  [0, 0, 393, 0, 393]\n",
            "0\n",
            "isNext :  False\n",
            "predict isNext :  False\n"
          ]
        }
      ],
      "source": [
        "# Predict mask tokens ans isNext\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[2]))\n",
        "print([id2word[w.item()] for w in input_ids[0] if id2word[w.item()] != '[PAD]'])\n",
        "\n",
        "logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)\n",
        "\n",
        "#predict masked tokens\n",
        "#max the probability along the vocab dim (2), [1] is the indices of the max, and [0] is the first value\n",
        "logits_lm = logits_lm.data.max(2)[1][0].data.numpy() \n",
        "#note that zero is padding we add to the masked_tokens\n",
        "print('masked tokens (words) : ',[id2word[pos.item()] for pos in masked_tokens[0]])\n",
        "print('masked tokens list : ',[pos.item() for pos in masked_tokens[0]])\n",
        "print('predict masked tokens (words) : ',[id2word[pos.item()] for pos in logits_lm])\n",
        "print('predict masked tokens list : ', [pos for pos in logits_lm])\n",
        "\n",
        "#predict nsp\n",
        "logits_nsp = logits_nsp.data.max(1)[1][0].data.numpy()\n",
        "print(logits_nsp)\n",
        "print('isNext : ', True if isNext else False)\n",
        "print('predict isNext : ',True if logits_nsp else False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'premise': Value(dtype='string', id=None),\n",
              " 'hypothesis': Value(dtype='string', id=None),\n",
              " 'label': ClassLabel(names=['entailment', 'neutral', 'contradiction'], id=None),\n",
              " 'idx': Value(dtype='int32', id=None)}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load the MNLI dataset\n",
        "import datasets\n",
        "\n",
        "mnli = datasets.load_dataset('glue', 'mnli')\n",
        "mnli['train'].features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['train', 'validation_matched', 'validation_mismatched', 'test_matched', 'test_mismatched'])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# list of datasets to remove 'idx' column from\n",
        "mnli.column_names.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# remove 'idx' column from each dataset\n",
        "for column_names in mnli.column_names.keys():\n",
        "    mnli[column_names] = mnli[column_names].remove_columns('idx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['train', 'validation_matched', 'validation_mismatched', 'test_matched', 'test_mismatched'])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# list of datasets to ensure that 'idx' column is removed\n",
        "mnli.column_names.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1, 2])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# list all label that have in the dataset\n",
        "np.unique(mnli['train']['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label'],\n",
              "        num_rows: 3000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label'],\n",
              "        num_rows: 500\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create dataset dictionary with sample data (since my computer cannot run all dataset)\n",
        "from datasets import DatasetDict\n",
        "\n",
        "raw_dataset = DatasetDict({\n",
        "    'train': mnli['train'].shuffle(seed=55).select(list(range(3000))),\n",
        "    'test': mnli['test_mismatched'].shuffle(seed=55).select(list(range(500))),\n",
        "    'validation': mnli['validation_mismatched'].shuffle(seed=55).select(list(range(1000)))\n",
        "})\n",
        "\n",
        "raw_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 3000/3000 [00:01<00:00, 2814.75 examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 2819.06 examples/s]\n",
            "Map: 100%|██████████| 1000/1000 [00:00<00:00, 3128.18 examples/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def preprocess_function(examples):\n",
        "    lst_input_ids_premise = []\n",
        "    lst_input_ids_hypothesis = []\n",
        "    lst_masked_tokens_premise = []\n",
        "    lst_masked_pos_premise = []\n",
        "    lst_masked_tokens_hypothesis = []\n",
        "    lst_masked_pos_hypothesis = []\n",
        "    lst_segment_ids = []\n",
        "    lst_attention_premise=[]\n",
        "    lst_attention_hypothesis=[]\n",
        "    labels = examples['label']\n",
        "    max_seq_length = 200\n",
        "    seed(55) \n",
        "    for i in range(len(examples['premise'])):\n",
        "\n",
        "        # convert the word to numeric\n",
        "        tokens_premise, tokens_hypothesis            = [word2id[word] if word in word_list else len(word_list) for word in examples['premise'][i].split()], \\\n",
        "                                                    [word2id[word] if word in word_list else len(word_list) for word in examples['hypothesis'][i].split()]\n",
        "        \n",
        "        #1. token embedding - add CLS and SEP on beginning and ending of premise and hypothesis\n",
        "        input_ids_premise = [word2id['[CLS]']] + tokens_premise + [word2id['[SEP]']]\n",
        "        input_ids_hypothesis = [word2id['[CLS]']] + tokens_hypothesis + [word2id['[SEP]']]\n",
        "      \n",
        "        #2. segment embedding - there one sentence so I decide to segment it as all 0\n",
        "        segment_ids = [0] * max_seq_length\n",
        "        #3 masking\n",
        "        n_pred_premise = min(max_mask, max(1, int(round(len(input_ids_premise) * 0.15))))\n",
        "\n",
        "        #get all the pos excluding CLS and SEP\n",
        "        candidates_masked_pos_premise = [i for i, token in enumerate(input_ids_premise) if token != word2id['[CLS]'] \n",
        "                                 and token != word2id['[SEP]']]\n",
        "        shuffle(candidates_masked_pos_premise)\n",
        "        masked_tokens_premise, masked_pos_premise = [], [] #compare the output with masked_tokens\n",
        "        #simply loop and mask accordingly\n",
        "        for pos in candidates_masked_pos_premise[:n_pred_premise]:\n",
        "            masked_pos_premise.append(pos)\n",
        "            masked_tokens_premise.append(input_ids_premise[pos])\n",
        "           \n",
        "            if random() < 0.1:  #10% replace with random token\n",
        "                index = randint(0, vocab_size - 1)\n",
        "                input_ids_premise[pos] = word2id[id2word[index]]\n",
        "            elif random() < 0.8:  #80 replace with [MASK]\n",
        "                input_ids_premise[pos] = word2id['[MASK]']\n",
        "            else: \n",
        "                pass\n",
        "\n",
        "        n_pred_hypothesis = min(max_mask, max(1, int(round(len(input_ids_hypothesis) * 0.15))))\n",
        "        #get all the pos excluding CLS and SEP\n",
        "        candidates_masked_pos_hypothesis = [i for i, token in enumerate(input_ids_hypothesis) if token != word2id['[CLS]'] \n",
        "                                 and token != word2id['[SEP]']]\n",
        "        shuffle(candidates_masked_pos_hypothesis)\n",
        "        masked_tokens_hypothesis, masked_pos_hypothesis = [], [] #compare the output with masked_tokens\n",
        "        #simply loop and mask accordingly\n",
        "        for pos in candidates_masked_pos_hypothesis[:n_pred_hypothesis]:\n",
        "            masked_pos_hypothesis.append(pos)\n",
        "            masked_tokens_hypothesis.append(input_ids_hypothesis[pos])\n",
        "            if random() < 0.1:  #10% replace with random token\n",
        "                index = randint(0, vocab_size - 1)\n",
        "                input_ids_hypothesis[pos] = word2id[id2word[index]]\n",
        "            elif random() < 0.8:  #80 replace with [MASK]\n",
        "                input_ids_hypothesis[pos] = word2id['[MASK]']\n",
        "            else: \n",
        "                pass\n",
        "        \n",
        "        #4. pad the sentence to the max length\n",
        "        n_pad_premise = max_seq_length - len(input_ids_premise)\n",
        "        input_ids_premise.extend([0] * n_pad_premise)\n",
        "        \n",
        "        #5. pad the mask tokens to the max length\n",
        "        if max_mask > n_pred_premise:\n",
        "            n_pad_premise = max_mask - n_pred_premise\n",
        "            masked_tokens_premise.extend([0] * n_pad_premise)\n",
        "            masked_pos_premise.extend([0] * n_pad_premise)\n",
        "            attention_premise = [1]*n_pred_premise+[0]*(n_pad_premise)\n",
        "            \n",
        "        #4. pad the sentence to the max length\n",
        "        n_pad_hypothesis = max_seq_length - len(input_ids_hypothesis)\n",
        "        input_ids_hypothesis.extend([0] * n_pad_hypothesis)\n",
        "        \n",
        "        #5. pad the mask tokens to the max length\n",
        "        if max_mask > n_pred_hypothesis:\n",
        "            n_pad_hypothesis = max_mask - n_pred_hypothesis\n",
        "            masked_tokens_hypothesis.extend([0] * n_pad_hypothesis)\n",
        "            masked_pos_hypothesis.extend([0] * n_pad_hypothesis)\n",
        "            attention_hypothesis = [1]*n_pred_hypothesis+[0]*(n_pad_hypothesis)\n",
        "        \n",
        "        # add the value to own list\n",
        "        lst_input_ids_premise.append(input_ids_premise)\n",
        "        lst_input_ids_hypothesis.append(input_ids_hypothesis)\n",
        "        lst_segment_ids.append(segment_ids)\n",
        "        lst_masked_tokens_premise.append(masked_tokens_premise)\n",
        "        lst_masked_pos_premise.append(masked_pos_premise)\n",
        "        lst_masked_tokens_hypothesis.append(masked_tokens_hypothesis)\n",
        "        lst_masked_pos_hypothesis.append(masked_pos_hypothesis)\n",
        "        lst_attention_premise.append(attention_premise)\n",
        "        lst_attention_hypothesis.append(attention_hypothesis)\n",
        "\n",
        "    # return as a dictionary\n",
        "    return {\n",
        "        \"premise_input_ids\": lst_input_ids_premise,\n",
        "        \"premise_pos_mask\":lst_masked_pos_premise,\n",
        "        \"hypothesis_input_ids\": lst_input_ids_hypothesis,\n",
        "        \"hypothesis_pos_mask\": lst_masked_pos_hypothesis,\n",
        "        \"segment_ids\": lst_segment_ids,\n",
        "        \"attention_premise\": lst_attention_premise,\n",
        "        \"attention_hypothesis\": lst_attention_hypothesis,\n",
        "        \"labels\" : labels,\n",
        "    }\n",
        "\n",
        "# map raw dataset with preprocess_function to create new data dict\n",
        "tokenized_datasets = raw_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        ")\n",
        "\n",
        "tokenized_datasets = tokenized_datasets.remove_columns(['premise','hypothesis','label'])\n",
        "tokenized_datasets.set_format(\"torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['premise_input_ids', 'premise_pos_mask', 'hypothesis_input_ids', 'hypothesis_pos_mask', 'segment_ids', 'attention_premise', 'attention_hypothesis', 'labels'],\n",
              "        num_rows: 3000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['premise_input_ids', 'premise_pos_mask', 'hypothesis_input_ids', 'hypothesis_pos_mask', 'segment_ids', 'attention_premise', 'attention_hypothesis', 'labels'],\n",
              "        num_rows: 500\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['premise_input_ids', 'premise_pos_mask', 'hypothesis_input_ids', 'hypothesis_pos_mask', 'segment_ids', 'attention_premise', 'attention_hypothesis', 'labels'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# create the dataloader\n",
        "batch_size = 32\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_datasets['train'], \n",
        "    batch_size=batch_size, \n",
        "    shuffle=True\n",
        ")\n",
        "eval_dataloader = DataLoader(\n",
        "    tokenized_datasets['validation'], \n",
        "    batch_size=batch_size\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    tokenized_datasets['test'], \n",
        "    batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 200])\n",
            "torch.Size([32, 5])\n",
            "torch.Size([32, 200])\n",
            "torch.Size([32, 5])\n",
            "torch.Size([32, 200])\n",
            "torch.Size([32, 5])\n",
            "torch.Size([32, 5])\n",
            "torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "# print the shape of each key \n",
        "for batch in train_dataloader:\n",
        "    print(batch['premise_input_ids'].shape)\n",
        "    print(batch['premise_pos_mask'].shape)\n",
        "    print(batch['hypothesis_input_ids'].shape)\n",
        "    print(batch['hypothesis_pos_mask'].shape)\n",
        "    print(batch['segment_ids'].shape)\n",
        "    print(batch['attention_premise'].shape)\n",
        "    print(batch['attention_hypothesis'].shape)\n",
        "    print(batch['labels'].shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load model from task1\n",
        "model1 = BERT()\n",
        "model1.load_state_dict(torch.load('models/best-bert-model.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define mean pooling function\n",
        "def mean_pool(token_embeds, attention_mask):\n",
        "    # reshape attention_mask to cover 768-dimension embeddings\n",
        "    in_mask = attention_mask.unsqueeze(-1).expand(\n",
        "        token_embeds.size()\n",
        "    ).float()\n",
        "    # perform mean-pooling but exclude padding tokens (specified by in_mask)\n",
        "    pool = torch.sum(token_embeds * in_mask, 1) / torch.clamp(\n",
        "        in_mask.sum(1), min=1e-9\n",
        "    )\n",
        "    return pool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# the function is for Classification Objective\n",
        "def configurations(u,v):\n",
        "    # build the |u-v| tensor\n",
        "    uv = torch.sub(u, v)   # batch_size,hidden_dim\n",
        "    uv_abs = torch.abs(uv) # batch_size,hidden_dim\n",
        "    \n",
        "    # concatenate u, v, |u-v|\n",
        "    x = torch.cat([u, v, uv_abs], dim=-1) # batch_size, 3*hidden_dim\n",
        "    return x\n",
        "\n",
        "# the function is for Regression Objective\n",
        "def cosine_similarity(u, v):\n",
        "    dot_product = np.dot(u, v)\n",
        "    norm_u = np.linalg.norm(u)\n",
        "    norm_v = np.linalg.norm(v)\n",
        "    similarity = dot_product / (norm_u * norm_v)\n",
        "    return similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "# classifier_head has shape (vocab_size*3,3)\n",
        "classifier_head = torch.nn.Linear(1546*3, 3).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model1.parameters(), lr=2e-5)\n",
        "optimizer_classifier = torch.optim.Adam(classifier_head.parameters(), lr=2e-5)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\earth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        }
      ],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# and setup a warmup for the first ~10% steps\n",
        "total_steps = int(len(raw_dataset) / batch_size)\n",
        "warmup_steps = int(0.1 * total_steps)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "\t\toptimizer, num_warmup_steps=warmup_steps,\n",
        "  \tnum_training_steps=total_steps - warmup_steps\n",
        ")\n",
        "\n",
        "# then during the training loop we update the scheduler per step\n",
        "scheduler.step()\n",
        "\n",
        "scheduler_classifier = get_linear_schedule_with_warmup(\n",
        "\t\toptimizer_classifier, num_warmup_steps=warmup_steps,\n",
        "  \tnum_training_steps=total_steps - warmup_steps\n",
        ")\n",
        "\n",
        "# then during the training loop we update the scheduler per step\n",
        "scheduler_classifier.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training the model1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 94/94 [15:52<00:00, 10.13s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 | loss = 3.849288\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 94/94 [16:41<00:00, 10.66s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2 | loss = 3.948837\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 94/94 [16:35<00:00, 10.59s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 3 | loss = 2.719974\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 94/94 [16:30<00:00, 10.53s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 4 | loss = 2.783654\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 94/94 [16:08<00:00, 10.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 5 | loss = 2.899947\n",
            "Time: 81m 48s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "num_epoch = 5\n",
        "# 1 epoch should be enough, increase if wanted\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epoch):\n",
        "    model1.train()  \n",
        "    classifier_head.train()\n",
        "    best_loss = float('inf')\n",
        "    # initialize the dataloader loop with tqdm (tqdm == progress bar)\n",
        "    for step, batch in enumerate(tqdm(train_dataloader, leave=True)):\n",
        "        # zero all gradients on each new step\n",
        "        optimizer.zero_grad()\n",
        "        optimizer_classifier.zero_grad()\n",
        "        \n",
        "        # prepare batches and more all to the active device\n",
        "        inputs_ids_a = batch['premise_input_ids'].to(device)\n",
        "        inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
        "        pos_mask_a = batch['premise_pos_mask'].to(device)\n",
        "        pos_mask_b = batch['hypothesis_pos_mask'].to(device)\n",
        "        segment_ids = batch['segment_ids'].to(device)\n",
        "        attention_a = batch['attention_premise'].to(device)\n",
        "        attention_b = batch['attention_hypothesis'].to(device)\n",
        "        label = batch['labels'].to(device)\n",
        "        \n",
        "        # extract token embeddings from BERT at last_hidden_state\n",
        "        u, _ = model1(inputs_ids_a, segment_ids, pos_mask_a)  \n",
        "        v, _ = model1(inputs_ids_b, segment_ids, pos_mask_b)  \n",
        "    \n",
        "\n",
        "        u_last_hidden_state = u # all token embeddings A = batch_size, seq_len, hidden_dim\n",
        "        v_last_hidden_state = v # all token embeddings B = batch_size, seq_len, hidden_dim\n",
        "\n",
        "         # get the mean pooled vectors\n",
        "        u_mean_pool = mean_pool(u_last_hidden_state, attention_a) # batch_size, hidden_dim\n",
        "        v_mean_pool = mean_pool(v_last_hidden_state, attention_b) # batch_size, hidden_dim\n",
        "        \n",
        "        # build the |u-v| tensor\n",
        "        uv = torch.sub(u_mean_pool, v_mean_pool)   # batch_size,hidden_dim\n",
        "        uv_abs = torch.abs(uv) # batch_size,hidden_dim\n",
        "        \n",
        "        # concatenate u, v, |u-v|\n",
        "        x = torch.cat([u_mean_pool, v_mean_pool, uv_abs], dim=-1) # batch_size, 3*hidden_dim\n",
        "        \n",
        "        # process concatenated tensor through classifier_head\n",
        "        x = classifier_head(x) #batch_size, classifer\n",
        "        \n",
        "        # calculate the 'softmax-loss' between predicted and true label\n",
        "        loss = criterion(x, label)\n",
        "        \n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            torch.save(model1.state_dict(), 'models/trained-model1.pt')\n",
        "        \n",
        "        # using loss, calculate gradients and then optimizerize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer_classifier.step()\n",
        "\n",
        "        scheduler.step() # update learning rate scheduler\n",
        "        scheduler_classifier.step()\n",
        "        \n",
        "    print(f'Epoch: {epoch + 1} | loss = {loss.item():.6f}')\n",
        "end_time = time.time()\n",
        "epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "print(f'Time: {epoch_mins}m {epoch_secs}s')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function for calculate the total parameters\n",
        "def count_parameters(model):\n",
        "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
        "    print(f'______\\n{sum(params):>6}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create function for compute the loss of model1 from task1 and task2\n",
        "def calculate_loss_model1(model, classifier, criterion, eval_dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(eval_dataloader):\n",
        "\n",
        "            inputs_ids_a = batch['premise_input_ids'].to(device)\n",
        "            inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
        "            pos_mask_a = batch['premise_pos_mask'].to(device)\n",
        "            pos_mask_b = batch['hypothesis_pos_mask'].to(device)\n",
        "            segment_ids = batch['segment_ids'].to(device)\n",
        "            attention_a = batch['attention_premise'].to(device)\n",
        "            attention_b = batch['attention_hypothesis'].to(device)\n",
        "            label = batch['labels'].to(device)\n",
        "\n",
        "            # extract token embeddings from BERT at last_hidden_state\n",
        "            u, _ = model(inputs_ids_a, segment_ids, pos_mask_a)  # all token embeddings A = batch_size, seq_len, hidden_dim\n",
        "            v, _ = model(inputs_ids_b, segment_ids, pos_mask_b)  # all token embeddings B = batch_size, seq_len, hidden_dim\n",
        "\n",
        "            # get the mean pooled vectors\n",
        "            u_mean_pool = mean_pool(u, attention_a) # batch_size, hidden_dim\n",
        "            v_mean_pool = mean_pool(v, attention_b) # batch_size, hidden_dim\n",
        "\n",
        "            # build the |u-v| tensor\n",
        "            uv = torch.sub(u_mean_pool, v_mean_pool)   # batch_size,hidden_dim\n",
        "            uv_abs = torch.abs(uv) # batch_size,hidden_dim\n",
        "            \n",
        "            # concatenate u, v, |u-v|\n",
        "            x = torch.cat([u_mean_pool, v_mean_pool, uv_abs], dim=-1) # batch_size, 3*hidden_dim\n",
        "            \n",
        "            # process concatenated tensor through classifier_head\n",
        "            x = classifier(x) #batch_size, classifer\n",
        "            \n",
        "            # calculate the 'softmax-loss' between predicted and true label\n",
        "            loss = criterion(x, label)\n",
        "\n",
        "            total_loss += loss\n",
        "    \n",
        "    average_loss = total_loss/len(eval_dataloader)\n",
        "    print(f\"Average Loss: {average_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create function for compute the cosine similarity of model1 from task1 and task2\n",
        "def calculate_cosine_sim_model1(model, classifier,eval_dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    total_similarity = 0\n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(eval_dataloader):\n",
        "            # prepare batches and more all to the active device\n",
        "            inputs_ids_a = batch['premise_input_ids'].to(device)\n",
        "            inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
        "            pos_mask_a = batch['premise_pos_mask'].to(device)\n",
        "            pos_mask_b = batch['hypothesis_pos_mask'].to(device)\n",
        "            segment_ids = batch['segment_ids'].to(device)\n",
        "            attention_a = batch['attention_premise'].to(device)\n",
        "            attention_b = batch['attention_hypothesis'].to(device)\n",
        "            label = batch['labels'].to(device)\n",
        "\n",
        "            # extract token embeddings from BERT at last_hidden_state\n",
        "\n",
        "            u, _ = model(inputs_ids_a, segment_ids, pos_mask_a)  \n",
        "            v, _ = model(inputs_ids_b, segment_ids, pos_mask_b) \n",
        "            # get the mean pooled vectors\n",
        "            u_mean_pool = mean_pool(u, attention_a).detach().cpu().numpy().reshape(-1) # batch_size, hidden_dim\n",
        "            v_mean_pool = mean_pool(v, attention_b).detach().cpu().numpy().reshape(-1) # batch_size, hidden_dim\n",
        "\n",
        "            similarity_score = cosine_similarity(u_mean_pool.reshape(1, -1), v_mean_pool.reshape(1, -1))[0, 0]\n",
        "            total_similarity += similarity_score\n",
        "        \n",
        "    average_similarity = total_similarity / len(eval_dataloader)\n",
        "    print(f\"Average Cosine Similarity: {average_similarity:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenize the sentence of model 1\n",
        "def tokenize_sentence_model1(sentence_a, sentence_b):\n",
        "    lst_input_ids_premise = []\n",
        "    lst_input_ids_hypothesis = []\n",
        "    lst_masked_tokens_premise = []\n",
        "    lst_masked_pos_premise = []\n",
        "    lst_masked_tokens_hypothesis = []\n",
        "    lst_masked_pos_hypothesis = []\n",
        "    lst_segment_ids = []\n",
        "    lst_attention_premise=[]\n",
        "    lst_attention_hypothesis=[]\n",
        "    max_seq_length = 200\n",
        "    seed(55) \n",
        "\n",
        "    tokens_premise, tokens_hypothesis            = [word2id[word] if word in word_list else len(word_list) for word in sentence_a.split()], \\\n",
        "                                                    [word2id[word] if word in word_list else len(word_list) for word in sentence_b.split()]\n",
        "    \n",
        "    input_ids_premise = [word2id['[CLS]']] + tokens_premise + [word2id['[SEP]']]\n",
        "    input_ids_hypothesis = [word2id['[CLS]']] + tokens_hypothesis + [word2id['[SEP]']]\n",
        "    \n",
        "    #2. segment embedding \n",
        "    segment_ids = [0] * max_seq_length\n",
        "     #3 masking\n",
        "    n_pred_premise = min(max_mask, max(1, int(round(len(input_ids_premise) * 0.15))))\n",
        "\n",
        "    #get all the pos excluding CLS and SEP\n",
        "    candidates_masked_pos_premise = [i for i, token in enumerate(input_ids_premise) if token != word2id['[CLS]'] \n",
        "                                 and token != word2id['[SEP]']]\n",
        "    shuffle(candidates_masked_pos_premise)\n",
        "    masked_tokens_premise, masked_pos_premise = [], [] #compare the output with masked_tokens\n",
        "    #simply loop and mask accordingly\n",
        "    for pos in candidates_masked_pos_premise[:n_pred_premise]:\n",
        "        masked_pos_premise.append(pos)\n",
        "        masked_tokens_premise.append(input_ids_premise[pos])\n",
        "           \n",
        "        if random() < 0.1:  #10% replace with random token\n",
        "            index = randint(0, vocab_size - 1)\n",
        "            input_ids_premise[pos] = word2id[id2word[index]]\n",
        "        elif random() < 0.8:  #80 replace with [MASK]\n",
        "            input_ids_premise[pos] = word2id['[MASK]']\n",
        "        else: \n",
        "            pass\n",
        "\n",
        "    n_pred_hypothesis = min(max_mask, max(1, int(round(len(input_ids_hypothesis) * 0.15))))\n",
        "    #get all the pos excluding CLS and SEP\n",
        "    candidates_masked_pos_hypothesis = [i for i, token in enumerate(input_ids_hypothesis) if token != word2id['[CLS]'] \n",
        "                                 and token != word2id['[SEP]']]\n",
        "    shuffle(candidates_masked_pos_hypothesis)\n",
        "    masked_tokens_hypothesis, masked_pos_hypothesis = [], [] #compare the output with masked_tokens\n",
        "    #simply loop and mask accordingly\n",
        "    for pos in candidates_masked_pos_hypothesis[:n_pred_hypothesis]:\n",
        "        masked_pos_hypothesis.append(pos)\n",
        "        masked_tokens_hypothesis.append(input_ids_hypothesis[pos])\n",
        "        if random() < 0.1:  #10% replace with random token\n",
        "            index = randint(0, vocab_size - 1)\n",
        "            input_ids_hypothesis[pos] = word2id[id2word[index]]\n",
        "        elif random() < 0.8:  #80 replace with [MASK]\n",
        "            input_ids_hypothesis[pos] = word2id['[MASK]']\n",
        "        else: \n",
        "            pass\n",
        "\n",
        "    #4. pad the sentence to the max length\n",
        "    n_pad_premise = max_seq_length - len(input_ids_premise)\n",
        "    input_ids_premise.extend([0] * n_pad_premise)\n",
        "        \n",
        "    #5. pad the mask tokens to the max length\n",
        "    if max_mask > n_pred_premise:\n",
        "        n_pad_premise = max_mask - n_pred_premise\n",
        "        masked_tokens_premise.extend([0] * n_pad_premise)\n",
        "        masked_pos_premise.extend([0] * n_pad_premise)\n",
        "        attention_premise = [1]*n_pred_premise+[0]*(n_pad_premise)\n",
        "            \n",
        "    #4. pad the sentence to the max length\n",
        "    n_pad_hypothesis = max_seq_length - len(input_ids_hypothesis)\n",
        "    input_ids_hypothesis.extend([0] * n_pad_hypothesis)\n",
        "        \n",
        "    #5. pad the mask tokens to the max length\n",
        "    if max_mask > n_pred_hypothesis:\n",
        "        n_pad_hypothesis = max_mask - n_pred_hypothesis\n",
        "        masked_tokens_hypothesis.extend([0] * n_pad_hypothesis)\n",
        "        masked_pos_hypothesis.extend([0] * n_pad_hypothesis)\n",
        "        attention_hypothesis = [1]*n_pred_hypothesis+[0]*(n_pad_hypothesis)\n",
        "\n",
        "    lst_input_ids_premise.append(input_ids_premise)\n",
        "    lst_input_ids_hypothesis.append(input_ids_hypothesis)\n",
        "    lst_segment_ids.append(segment_ids)\n",
        "    lst_masked_tokens_premise.append(masked_tokens_premise)\n",
        "    lst_masked_pos_premise.append(masked_pos_premise)\n",
        "    lst_masked_tokens_hypothesis.append(masked_tokens_hypothesis)\n",
        "    lst_masked_pos_hypothesis.append(masked_pos_hypothesis)\n",
        "    lst_attention_premise.append(attention_premise)\n",
        "    lst_attention_hypothesis.append(attention_hypothesis)\n",
        "\n",
        "    return {\n",
        "        \"premise_input_ids\": lst_input_ids_premise,\n",
        "        \"premise_pos_mask\":lst_masked_pos_premise,\n",
        "        \"hypothesis_input_ids\": lst_input_ids_hypothesis,\n",
        "        \"hypothesis_pos_mask\": lst_masked_pos_hypothesis,\n",
        "        \"segment_ids\": lst_segment_ids,\n",
        "        \"attention_premise\": lst_attention_premise,\n",
        "        \"attention_hypothesis\": lst_attention_hypothesis,\n",
        "        \n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# create function for compute consine similarity of unseen 2 sentence \n",
        "def calculate_similarity_model1(model, sentence_a, sentence_b, device):\n",
        "    # Tokenize and convert sentences to input IDs and attention masks\n",
        "    inputs = tokenize_sentence_model1(sentence_a, sentence_b)\n",
        "    \n",
        "    # Move input IDs and attention masks to the active device\n",
        "    inputs_ids_a = torch.tensor(inputs['premise_input_ids'])\n",
        "    pos_mask_a = torch.tensor(inputs['premise_pos_mask'])\n",
        "    attention_a = torch.tensor(inputs['attention_premise'])\n",
        "    inputs_ids_b = torch.tensor(inputs['hypothesis_input_ids'])\n",
        "    pos_mask_b = torch.tensor(inputs['hypothesis_pos_mask'])\n",
        "    attention_b = torch.tensor(inputs['attention_hypothesis'])\n",
        "    segment = torch.tensor(inputs['segment_ids'])\n",
        "\n",
        "    # Extract token embeddings from BERT\n",
        "    u,_ = model(inputs_ids_a, segment, pos_mask_a)  \n",
        "    v,_ = model(inputs_ids_b, segment, pos_mask_b) \n",
        "\n",
        "    # Get the mean-pooled vectors\n",
        "    u = mean_pool(u, attention_a).detach().cpu().numpy().reshape(-1)  \n",
        "    v = mean_pool(v, attention_b).detach().cpu().numpy().reshape(-1)  \n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity_score = cosine_similarity(u.reshape(1, -1), v.reshape(1, -1))[0, 0]\n",
        "\n",
        "    return similarity_score   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate model1 before training with MNLI dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load model1 before retrain in task2 \n",
        "model1 = BERT()\n",
        "model1.load_state_dict(torch.load('models/best-bert-model.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "______\n",
            "37951500\n"
          ]
        }
      ],
      "source": [
        "count_parameters(model1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Cosine Similarity: 0.9999\n"
          ]
        }
      ],
      "source": [
        "calculate_cosine_sim_model1(model1,classifier_head,eval_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Loss: 16.4644\n"
          ]
        }
      ],
      "source": [
        "calculate_loss_model1(model1,classifier_head,criterion,eval_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine Similarity: 0.9999\n"
          ]
        }
      ],
      "source": [
        "sentence_a = 'Your contribution helped make it possible for us to provide our students with a quality education.'\n",
        "sentence_b = \"Your contributions were of no help with our students' education.\"\n",
        "similarity = calculate_similarity_model1(model1, sentence_a, sentence_b, device)\n",
        "print(f\"Cosine Similarity: {similarity:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate model1 after training with MNLI dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Instantiate the BERT model\n",
        "saved_model1 = BERT()\n",
        "saved_model1.load_state_dict(torch.load('models/trained-model1.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Cosine Similarity: 0.9999\n"
          ]
        }
      ],
      "source": [
        "calculate_cosine_sim_model1(saved_model1,classifier_head,eval_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Loss: 16.4727\n"
          ]
        }
      ],
      "source": [
        "calculate_loss_model1(saved_model1,classifier_head,criterion,eval_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine Similarity: 1.0000\n"
          ]
        }
      ],
      "source": [
        "sentence_a = 'Your contribution helped make it possible for us to provide our students with a quality education.'\n",
        "sentence_b = \"Your contributions were of no help with our students' education.\"\n",
        "similarity = calculate_similarity_model1(saved_model1, sentence_a, sentence_b, device)\n",
        "print(f\"Cosine Similarity: {similarity:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create function to evaluate model2 and model3 with validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create function to compute cosine similarity of model2 and model3\n",
        "def calculate_cosine_sim2(model, classifier_head, eval_dataloader):\n",
        "    model.eval()\n",
        "    classifier_head.eval()\n",
        "    total_similarity = 0\n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(eval_dataloader):\n",
        "            # prepare batches and more all to the active device\n",
        "            inputs_ids_a = batch['premise_input_ids'].to(device)\n",
        "            inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
        "            attention_a = batch['premise_attention_mask'].to(device)\n",
        "            attention_b = batch['hypothesis_attention_mask'].to(device)\n",
        "            label = batch['labels'].to(device)\n",
        "            \n",
        "            # extract token embeddings from BERT at last_hidden_state\n",
        "            u = model(inputs_ids_a, attention_mask=attention_a)[0]  # all token embeddings A = batch_size, seq_len, hidden_dim\n",
        "            v = model(inputs_ids_b, attention_mask=attention_b)[0]  # all token embeddings B = batch_size, seq_len, hidden_dim\n",
        "\n",
        "            # get the mean pooled vectors\n",
        "            u_mean_pool = mean_pool(u, attention_a).detach().cpu().numpy().reshape(-1) # batch_size, hidden_dim\n",
        "            v_mean_pool = mean_pool(v, attention_b).detach().cpu().numpy().reshape(-1) # batch_size, hidden_dim\n",
        "\n",
        "            similarity_score = cosine_similarity(u_mean_pool.reshape(1, -1), v_mean_pool.reshape(1, -1))[0, 0]\n",
        "            total_similarity += similarity_score\n",
        "        \n",
        "    average_similarity = total_similarity / len(eval_dataloader)\n",
        "    print(f\"Average Cosine Similarity: {average_similarity:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create function to compute loss of model2 and model3\n",
        "def calculate_loss2(model, classifier_head,criterion, eval_dataloader):\n",
        "    model.eval()\n",
        "    classifier_head.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(eval_dataloader):\n",
        "            # prepare batches and more all to the active device\n",
        "            inputs_ids_a = batch['premise_input_ids'].to(device)\n",
        "            inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
        "            attention_a = batch['premise_attention_mask'].to(device)\n",
        "            attention_b = batch['hypothesis_attention_mask'].to(device)\n",
        "            label = batch['labels'].to(device)\n",
        "        \n",
        "            # extract token embeddings from BERT at last_hidden_state\n",
        "            u = model(inputs_ids_a, attention_mask=attention_a)  # all token embeddings A = batch_size, seq_len, hidden_dim\n",
        "            v = model(inputs_ids_b, attention_mask=attention_b) # all token embeddings B = batch_size, seq_len, hidden_dim \n",
        "\n",
        "            u_last_hidden_state = u.last_hidden_state # all token embeddings A = batch_size, seq_len, hidden_dim\n",
        "            v_last_hidden_state = v.last_hidden_state # all token embeddings B = batch_size, seq_len, hidden_dim\n",
        "\n",
        "            # get the mean pooled vectors\n",
        "            u_mean_pool = mean_pool(u_last_hidden_state, attention_a) # batch_size, hidden_dim\n",
        "            v_mean_pool = mean_pool(v_last_hidden_state, attention_b) # batch_size, hidden_dim\n",
        "        \n",
        "            # build the |u-v| tensor\n",
        "            uv = torch.sub(u_mean_pool, v_mean_pool)   # batch_size,hidden_dim\n",
        "            uv_abs = torch.abs(uv) # batch_size,hidden_dim\n",
        "        \n",
        "            # concatenate u, v, |u-v|\n",
        "            x = torch.cat([u_mean_pool, v_mean_pool, uv_abs], dim=-1) # batch_size, 3*hidden_dim\n",
        "        \n",
        "            # process concatenated tensor through classifier_head\n",
        "            x = classifier_head(x) #batch_size, classifer\n",
        "        \n",
        "            # calculate the 'softmax-loss' between predicted and true label\n",
        "            loss = criterion(x, label)\n",
        "            total_loss += loss\n",
        "            \n",
        "    average_loss = total_loss / len(eval_dataloader)\n",
        "    print(f\"Average Loss: {average_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# create function to compute cosine similarity on 2 unseen sentence on model2 and model3\n",
        "def calculate_similarity2(model, tokenizer, sentence_a, sentence_b, device):\n",
        "    # Tokenize and convert sentences to input IDs and attention masks\n",
        "    inputs_a = tokenizer(sentence_a, return_tensors='pt', truncation=True, padding=True).to(device)\n",
        "    inputs_b = tokenizer(sentence_b, return_tensors='pt', truncation=True, padding=True).to(device)\n",
        "\n",
        "    # Move input IDs and attention masks to the active device\n",
        "    inputs_ids_a = inputs_a['input_ids']\n",
        "    attention_a = inputs_a['attention_mask']\n",
        "    inputs_ids_b = inputs_b['input_ids']\n",
        "    attention_b = inputs_b['attention_mask']\n",
        "\n",
        "    # Extract token embeddings from BERT\n",
        "    u = model(inputs_ids_a, attention_mask=attention_a)[0]  # all token embeddings A = batch_size, seq_len, hidden_dim\n",
        "    v = model(inputs_ids_b, attention_mask=attention_b)[0]  # all token embeddings B = batch_size, seq_len, hidden_dim\n",
        "\n",
        "    # Get the mean-pooled vectors\n",
        "    u = mean_pool(u, attention_a).detach().cpu().numpy().reshape(-1)  # batch_size, hidden_dim\n",
        "    v = mean_pool(v, attention_b).detach().cpu().numpy().reshape(-1)  # batch_size, hidden_dim\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity_score = cosine_similarity(u.reshape(1, -1), v.reshape(1, -1))[0, 0]\n",
        "\n",
        "    return similarity_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the pretain tokenizer\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer_model2 = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenize the data\n",
        "def preprocess_function2(examples):\n",
        "    max_seq_length = 128\n",
        "    padding = 'max_length'\n",
        "    # Tokenize the premise\n",
        "    premise_result = tokenizer_model2(\n",
        "        examples['premise'], padding=padding, max_length=max_seq_length, truncation=True)\n",
        "    #num_rows, max_seq_length\n",
        "    # Tokenize the hypothesis\n",
        "    hypothesis_result = tokenizer_model2(\n",
        "        examples['hypothesis'], padding=padding, max_length=max_seq_length, truncation=True)\n",
        "    #num_rows, max_seq_length\n",
        "    # Extract labels\n",
        "    labels = examples[\"label\"]\n",
        "    #num_rows\n",
        "    return {\n",
        "        \"premise_input_ids\": premise_result[\"input_ids\"],\n",
        "        \"premise_attention_mask\": premise_result[\"attention_mask\"],\n",
        "        \"hypothesis_input_ids\": hypothesis_result[\"input_ids\"],\n",
        "        \"hypothesis_attention_mask\": hypothesis_result[\"attention_mask\"],\n",
        "        \"labels\" : labels\n",
        "    }\n",
        "\n",
        "tokenized_datasets2 = raw_dataset.map(\n",
        "    preprocess_function2,\n",
        "    batched=True,\n",
        ")\n",
        "\n",
        "tokenized_datasets2 = tokenized_datasets2.remove_columns(['premise','hypothesis','label'])\n",
        "tokenized_datasets2.set_format(\"torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['premise_input_ids', 'premise_attention_mask', 'hypothesis_input_ids', 'hypothesis_attention_mask', 'labels'],\n",
              "        num_rows: 3000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['premise_input_ids', 'premise_attention_mask', 'hypothesis_input_ids', 'hypothesis_attention_mask', 'labels'],\n",
              "        num_rows: 500\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['premise_input_ids', 'premise_attention_mask', 'hypothesis_input_ids', 'hypothesis_attention_mask', 'labels'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_datasets2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# initialize the dataloader\n",
        "batch_size = 32\n",
        "train_dataloader2 = DataLoader(\n",
        "    tokenized_datasets2['train'], \n",
        "    batch_size=batch_size, \n",
        "    shuffle=True\n",
        ")\n",
        "eval_dataloader2 = DataLoader(\n",
        "    tokenized_datasets2['validation'], \n",
        "    batch_size=batch_size\n",
        ")\n",
        "test_dataloader2 = DataLoader(\n",
        "    tokenized_datasets2['test'], \n",
        "    batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 128])\n",
            "torch.Size([32, 128])\n",
            "torch.Size([32, 128])\n",
            "torch.Size([32, 128])\n",
            "torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "for batch in train_dataloader2:\n",
        "    print(batch['premise_input_ids'].shape)\n",
        "    print(batch['premise_attention_mask'].shape)\n",
        "    print(batch['hypothesis_input_ids'].shape)\n",
        "    print(batch['hypothesis_attention_mask'].shape)\n",
        "    print(batch['labels'].shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# start from a pretrained bert-base-uncased model\n",
        "from transformers import BertTokenizer, BertModel\n",
        "model2 = BertModel.from_pretrained('bert-base-uncased')\n",
        "model2.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "______\n",
            "109482240\n"
          ]
        }
      ],
      "source": [
        "count_parameters(model2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [],
      "source": [
        "classifier_head2 = torch.nn.Linear(768*3, 3).to(device)\n",
        "\n",
        "optimizer2 = torch.optim.Adam(model2.parameters(), lr=2e-5)\n",
        "optimizer_classifier2 = torch.optim.Adam(classifier_head2.parameters(), lr=2e-5)\n",
        "\n",
        "criterion2 = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\earth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        }
      ],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# and setup a warmup for the first ~10% steps\n",
        "total_steps = int(len(raw_dataset) / batch_size)\n",
        "warmup_steps = int(0.1 * total_steps)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "\t\toptimizer2, num_warmup_steps=warmup_steps,\n",
        "  \tnum_training_steps=total_steps - warmup_steps\n",
        ")\n",
        "\n",
        "# then during the training loop we update the scheduler per step\n",
        "scheduler.step()\n",
        "\n",
        "scheduler_classifier = get_linear_schedule_with_warmup(\n",
        "\t\toptimizer_classifier2, num_warmup_steps=warmup_steps,\n",
        "  \tnum_training_steps=total_steps - warmup_steps\n",
        ")\n",
        "\n",
        "# then during the training loop we update the scheduler per step\n",
        "scheduler_classifier.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate model2 before training with MNLI dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Cosine Similarity: 0.7733\n"
          ]
        }
      ],
      "source": [
        "calculate_cosine_sim2(model2,classifier_head2,eval_dataloader2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Loss: 1.1354\n"
          ]
        }
      ],
      "source": [
        "calculate_loss2(model2,classifier_head2,criterion2,eval_dataloader2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine Similarity: 0.8057\n"
          ]
        }
      ],
      "source": [
        "sentence_a = 'Your contribution helped make it possible for us to provide our students with a quality education.'\n",
        "sentence_b = \"Your contributions were of no help with our students' education.\"\n",
        "similarity = calculate_similarity2(model2, tokenizer_model2, sentence_a, sentence_b, device)\n",
        "print(f\"Cosine Similarity: {similarity:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training the model2 with MNLI dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 94/94 [18:33<00:00, 11.85s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 | loss = 1.037066\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 94/94 [19:05<00:00, 12.18s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2 | loss = 0.674230\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 94/94 [18:03<00:00, 11.53s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 3 | loss = 0.533247\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 94/94 [18:08<00:00, 11.58s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 4 | loss = 0.268627\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 94/94 [18:03<00:00, 11.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 5 | loss = 0.085280\n",
            "Time: 91m 54s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "num_epoch = 5\n",
        "start_time = time.time()\n",
        "best_loss = float('inf')\n",
        "# 1 epoch should be enough, increase if wanted\n",
        "for epoch in range(num_epoch):\n",
        "    model2.train()  \n",
        "    classifier_head2.train()\n",
        "    # initialize the dataloader loop with tqdm (tqdm == progress bar)\n",
        "    for step, batch in enumerate(tqdm(train_dataloader2, leave=True)):\n",
        "        # zero all gradients on each new step\n",
        "        optimizer2.zero_grad()\n",
        "        optimizer_classifier2.zero_grad()\n",
        "        \n",
        "        # prepare batches and more all to the active device\n",
        "        inputs_ids_a = batch['premise_input_ids'].to(device)\n",
        "        inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
        "        attention_a = batch['premise_attention_mask'].to(device)\n",
        "        attention_b = batch['hypothesis_attention_mask'].to(device)\n",
        "        label = batch['labels'].to(device)\n",
        "        \n",
        "        # extract token embeddings from BERT at last_hidden_state\n",
        "        u = model2(inputs_ids_a, attention_mask=attention_a)  \n",
        "        v = model2(inputs_ids_b, attention_mask=attention_b)  \n",
        "\n",
        "        \n",
        "        u_last_hidden_state = u.last_hidden_state # all token embeddings A = batch_size, seq_len, hidden_dim\n",
        "        v_last_hidden_state = v.last_hidden_state # all token embeddings B = batch_size, seq_len, hidden_dim\n",
        "\n",
        "         # get the mean pooled vectors\n",
        "        u_mean_pool = mean_pool(u_last_hidden_state, attention_a) # batch_size, hidden_dim\n",
        "        v_mean_pool = mean_pool(v_last_hidden_state, attention_b) # batch_size, hidden_dim\n",
        "        \n",
        "        # build the |u-v| tensor\n",
        "        uv = torch.sub(u_mean_pool, v_mean_pool)   # batch_size,hidden_dim\n",
        "        uv_abs = torch.abs(uv) # batch_size,hidden_dim\n",
        "        \n",
        "        # concatenate u, v, |u-v|\n",
        "        x = torch.cat([u_mean_pool, v_mean_pool, uv_abs], dim=-1) # batch_size, 3*hidden_dim\n",
        "        \n",
        "        # process concatenated tensor through classifier_head\n",
        "        x = classifier_head2(x) #batch_size, classifer\n",
        "        \n",
        "        # calculate the 'softmax-loss' between predicted and true label\n",
        "        loss = criterion2(x, label)\n",
        "\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            torch.save(model2.state_dict(), 'models/trained-model2.pt')\n",
        "        \n",
        "        # using loss, calculate gradients and then optimizerize\n",
        "        loss.backward()\n",
        "        optimizer2.step()\n",
        "        optimizer_classifier2.step()\n",
        "\n",
        "        scheduler.step() # update learning rate scheduler\n",
        "        scheduler_classifier.step()\n",
        "        \n",
        "    print(f'Epoch: {epoch + 1} | loss = {loss.item():.6f}')\n",
        "end_time = time.time()\n",
        "epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "print(f'Time: {epoch_mins}m {epoch_secs}s')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate model2 after training with MNLI dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "saved_model2 = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the state dictionary of your trained model into the new model\n",
        "saved_model2.load_state_dict(torch.load('models/trained-model2.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Cosine Similarity: 0.4133\n"
          ]
        }
      ],
      "source": [
        "calculate_cosine_sim2(saved_model2,classifier_head2,eval_dataloader2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Loss: 1.1322\n"
          ]
        }
      ],
      "source": [
        "calculate_loss2(saved_model2,classifier_head2,criterion2,eval_dataloader2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine Similarity: -0.0900\n"
          ]
        }
      ],
      "source": [
        "# Example usage:\n",
        "sentence_a = 'Your contribution helped make it possible for us to provide our students with a quality education.'\n",
        "sentence_b = \"Your contributions were of no help with our students' education.\"\n",
        "similarity = calculate_similarity2(saved_model2, tokenizer_model2, sentence_a, sentence_b, device)\n",
        "print(f\"Cosine Similarity: {similarity:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the pretain tokenizer\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "\n",
        "tokenizer_model3 = RobertaTokenizer.from_pretrained('roberta-base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenize the data\n",
        "def preprocess_function3(examples):\n",
        "    max_seq_length = 128\n",
        "    padding = 'max_length'\n",
        "    # Tokenize the premise\n",
        "    premise_result = tokenizer_model3(\n",
        "        examples['premise'], padding=padding, max_length=max_seq_length, truncation=True)\n",
        "    #num_rows, max_seq_length\n",
        "    # Tokenize the hypothesis\n",
        "    hypothesis_result = tokenizer_model3(\n",
        "        examples['hypothesis'], padding=padding, max_length=max_seq_length, truncation=True)\n",
        "    #num_rows, max_seq_length\n",
        "    # Extract labels\n",
        "    labels = examples[\"label\"]\n",
        "    #num_rows\n",
        "    return {\n",
        "        \"premise_input_ids\": premise_result[\"input_ids\"],\n",
        "        \"premise_attention_mask\": premise_result[\"attention_mask\"],\n",
        "        \"hypothesis_input_ids\": hypothesis_result[\"input_ids\"],\n",
        "        \"hypothesis_attention_mask\": hypothesis_result[\"attention_mask\"],\n",
        "        \"labels\" : labels\n",
        "    }\n",
        "\n",
        "tokenized_datasets3 = raw_dataset.map(\n",
        "    preprocess_function3,\n",
        "    batched=True,\n",
        ")\n",
        "\n",
        "tokenized_datasets3 = tokenized_datasets3.remove_columns(['premise','hypothesis','label'])\n",
        "tokenized_datasets3.set_format(\"torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['premise_input_ids', 'premise_attention_mask', 'hypothesis_input_ids', 'hypothesis_attention_mask', 'labels'],\n",
              "        num_rows: 3000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['premise_input_ids', 'premise_attention_mask', 'hypothesis_input_ids', 'hypothesis_attention_mask', 'labels'],\n",
              "        num_rows: 500\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['premise_input_ids', 'premise_attention_mask', 'hypothesis_input_ids', 'hypothesis_attention_mask', 'labels'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_datasets3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# create dataloader\n",
        "batch_size = 32\n",
        "train_dataloader3 = DataLoader(\n",
        "    tokenized_datasets3['train'], \n",
        "    batch_size=batch_size, \n",
        "    shuffle=True\n",
        ")\n",
        "eval_dataloader3 = DataLoader(\n",
        "    tokenized_datasets3['validation'], \n",
        "    batch_size=batch_size\n",
        ")\n",
        "test_dataloader3 = DataLoader(\n",
        "    tokenized_datasets3['test'], \n",
        "    batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 128])\n",
            "torch.Size([32, 128])\n",
            "torch.Size([32, 128])\n",
            "torch.Size([32, 128])\n",
            "torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "for batch in train_dataloader3:\n",
        "    print(batch['premise_input_ids'].shape)\n",
        "    print(batch['premise_attention_mask'].shape)\n",
        "    print(batch['hypothesis_input_ids'].shape)\n",
        "    print(batch['hypothesis_attention_mask'].shape)\n",
        "    print(batch['labels'].shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RobertaModel(\n",
              "  (embeddings): RobertaEmbeddings(\n",
              "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "    (token_type_embeddings): Embedding(1, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): RobertaEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): RobertaPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load the roberta pretrained model\n",
        "model3 = RobertaModel.from_pretrained('roberta-base')\n",
        "model3.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "______\n",
            "124645632\n"
          ]
        }
      ],
      "source": [
        "count_parameters(model3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [],
      "source": [
        "classifier_head3 = torch.nn.Linear(768*3, 3).to(device)\n",
        "\n",
        "optimizer3 = torch.optim.Adam(model3.parameters(), lr=2e-5)\n",
        "optimizer_classifier3 = torch.optim.Adam(classifier_head3.parameters(), lr=2e-5)\n",
        "\n",
        "criterion3 = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\earth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        }
      ],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# and setup a warmup for the first ~10% steps\n",
        "total_steps = int(len(raw_dataset) / batch_size)\n",
        "warmup_steps = int(0.1 * total_steps)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "\t\toptimizer, num_warmup_steps=warmup_steps,\n",
        "  \tnum_training_steps=total_steps - warmup_steps\n",
        ")\n",
        "\n",
        "# then during the training loop we update the scheduler per step\n",
        "scheduler.step()\n",
        "\n",
        "scheduler_classifier = get_linear_schedule_with_warmup(\n",
        "\t\toptimizer_classifier, num_warmup_steps=warmup_steps,\n",
        "  \tnum_training_steps=total_steps - warmup_steps\n",
        ")\n",
        "\n",
        "# then during the training loop we update the scheduler per step\n",
        "scheduler_classifier.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate model3 before training with MNLI dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Cosine Similarity: 0.9767\n"
          ]
        }
      ],
      "source": [
        "calculate_cosine_sim2(model3,classifier_head3,eval_dataloader3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Loss: 1.1290\n"
          ]
        }
      ],
      "source": [
        "calculate_loss2(model3,classifier_head3,criterion3,eval_dataloader2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine Similarity: 0.9762\n"
          ]
        }
      ],
      "source": [
        "sentence_a = 'Your contribution helped make it possible for us to provide our students with a quality education.'\n",
        "sentence_b = \"Your contributions were of no help with our students' education.\"\n",
        "similarity = calculate_similarity2(model3, tokenizer_model3, sentence_a, sentence_b, device)\n",
        "print(f\"Cosine Similarity: {similarity:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training model3 with MNLI dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 94/94 [18:21<00:00, 11.72s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 | loss = 1.027755\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 94/94 [18:03<00:00, 11.53s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2 | loss = 0.798118\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 94/94 [18:03<00:00, 11.52s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 3 | loss = 0.935967\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 94/94 [18:01<00:00, 11.51s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 4 | loss = 0.643016\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 94/94 [18:04<00:00, 11.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 5 | loss = 0.119918\n",
            "Time: 90m 35s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "num_epoch = 5\n",
        "start_time = time.time()\n",
        "best_loss = float('inf')\n",
        "# 1 epoch should be enough, increase if wanted\n",
        "for epoch in range(num_epoch):\n",
        "    model3.train()  \n",
        "    classifier_head3.train()\n",
        "    # initialize the dataloader loop with tqdm (tqdm == progress bar)\n",
        "    for step, batch in enumerate(tqdm(train_dataloader3, leave=True)):\n",
        "        # zero all gradients on each new step\n",
        "        optimizer3.zero_grad()\n",
        "        optimizer_classifier3.zero_grad()\n",
        "        \n",
        "        # prepare batches and more all to the active device\n",
        "        inputs_ids_a = batch['premise_input_ids'].to(device)\n",
        "        inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
        "        attention_a = batch['premise_attention_mask'].to(device)\n",
        "        attention_b = batch['hypothesis_attention_mask'].to(device)\n",
        "        label = batch['labels'].to(device)\n",
        "        \n",
        "        # extract token embeddings from BERT at last_hidden_state\n",
        "        u = model3(inputs_ids_a, attention_mask=attention_a)  \n",
        "        v = model3(inputs_ids_b, attention_mask=attention_b)  \n",
        "\n",
        "        \n",
        "        u_last_hidden_state = u.last_hidden_state # all token embeddings A = batch_size, seq_len, hidden_dim\n",
        "        v_last_hidden_state = v.last_hidden_state # all token embeddings B = batch_size, seq_len, hidden_dim\n",
        "\n",
        "         # get the mean pooled vectors\n",
        "        u_mean_pool = mean_pool(u_last_hidden_state, attention_a) # batch_size, hidden_dim\n",
        "        v_mean_pool = mean_pool(v_last_hidden_state, attention_b) # batch_size, hidden_dim\n",
        "        \n",
        "        # build the |u-v| tensor\n",
        "        uv = torch.sub(u_mean_pool, v_mean_pool)   # batch_size,hidden_dim\n",
        "        uv_abs = torch.abs(uv) # batch_size,hidden_dim\n",
        "        \n",
        "        # concatenate u, v, |u-v|\n",
        "        x = torch.cat([u_mean_pool, v_mean_pool, uv_abs], dim=-1) # batch_size, 3*hidden_dim\n",
        "        \n",
        "        # process concatenated tensor through classifier_head\n",
        "        x = classifier_head3(x) #batch_size, classifer\n",
        "        \n",
        "        # calculate the 'softmax-loss' between predicted and true label\n",
        "        loss = criterion3(x, label)\n",
        "\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            torch.save(model3.state_dict(), 'models/trained-model3.pt')\n",
        "        \n",
        "        # using loss, calculate gradients and then optimizerize\n",
        "        loss.backward()\n",
        "        optimizer3.step()\n",
        "        optimizer_classifier3.step()\n",
        "\n",
        "        scheduler.step() # update learning rate scheduler\n",
        "        scheduler_classifier.step()\n",
        "        \n",
        "    print(f'Epoch: {epoch + 1} | loss = {loss.item():.6f}')\n",
        "end_time = time.time()\n",
        "epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "print(f'Time: {epoch_mins}m {epoch_secs}s')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate model3 after training with MNLI dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 168,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Instantiate a new BERT model\n",
        "saved_model3 = RobertaModel.from_pretrained('roberta-base')\n",
        "\n",
        "# Load the state dictionary of your trained model into the new model\n",
        "saved_model3.load_state_dict(torch.load('models/trained-model3.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Cosine Similarity: 0.4778\n"
          ]
        }
      ],
      "source": [
        "calculate_cosine_sim2(saved_model3,classifier_head3,eval_dataloader3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Loss: 1.2201\n"
          ]
        }
      ],
      "source": [
        "calculate_loss2(saved_model3,classifier_head3,criterion3,eval_dataloader2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine Similarity: -0.0682\n"
          ]
        }
      ],
      "source": [
        "# Example usage:\n",
        "sentence_a = 'Your contribution helped make it possible for us to provide our students with a quality education.'\n",
        "sentence_b = \"Your contributions were of no help with our students' education.\"\n",
        "similarity = calculate_similarity2(saved_model3, tokenizer_model3, sentence_a, sentence_b, device)\n",
        "print(f\"Cosine Similarity: {similarity:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Model | trainable parameter | Average Cosine Similarity (before)| Average Loss (before)| Cosine Similarity with one specific pair sentence (before) | Average Cosine Similarity (after)| Average Loss (after)| Cosine Similarity with one specific pair sentence (after) | Training Time (train with MNLI dataset)\n",
        "|:------------------------|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:|\n",
        "| Model1 (my model) |    37,951,500   |    0.9999     |  16.4644 | 0.9999 |  0.9999 | 16.4727 | 1.0000 | 81m 48s|\n",
        "| Model2 (bert-base-uncased) |   109,482,240    |    0.7733 |  1.1354 | 0.8057 | 0.4133 | 1.1322 | -0.0900 | 91m 54s|  \n",
        "| Model3 (roberta-base)     |      124,645,632  |    0.9762    |  1.1290  |   0.9762    | 0.4778 | 1.2201 | -0.0682 | 90m 35s|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From result, you can see that when model train with 3,000 sample MNLI dataset, the loss of model is higher expert model2 that increase a lillle. I observed that model with high number trainable parameter is better than one with small number of trainable parameter in loss.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For hyperparameter, I think for model1, I can increase number of Encoder of Encoder Layer and number of heads in Multi-Head Attention to make model more complex and learn better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For limitation, my computer cannot train with huge dataset/text (). Moreover, I have not enough time to train more epoch (my GPU is not work). it may cause my model to look so bad. In the future, I can improve this 3 model by training more epoch and train with huge dataset."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
